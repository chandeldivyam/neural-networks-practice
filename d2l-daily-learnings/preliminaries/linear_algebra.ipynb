{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8b20e2-3d58-4b74-863b-70d1305ff7f0",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "We understand the importance of vectors, matrics and tensors and how to perform algebraic operations on them. Also, some relevant mathematical operations that will come handy during model preparation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5381a789-dff2-40a8-96d2-7274fb8174f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(3.)\n",
      "tensor([0, 1, 2]) torch.Size([3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x, y = torch.tensor(2), torch.tensor(3.)\n",
    "print(x, y) # Scalar\n",
    "\n",
    "x = torch.arange(3)\n",
    "print(x, x.shape) # Vectors\n",
    "\n",
    "X = torch.arange(6).view((2,3))\n",
    "print(X) # Matrics\n",
    "\n",
    "X_transpose = X.T\n",
    "print(X_transpose) # X_transpose_ij = X_ji\n",
    "\n",
    "X = torch.arange(24).view((2,3,4))\n",
    "print(X) # Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08c32-8205-49ad-a375-7926a4fc41c6",
   "metadata": {},
   "source": [
    "### Reduction and Non-Reduction of Tensor\n",
    "\n",
    "We can reduce the tensor along any axis with `sum()` function. Also, we can sum only along a particular axis and not reduce it completely.\n",
    "\n",
    "The `keepdims` parameter keeps the dimensions along differents axes, this helps when we want to normallize along some direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d0e31c9-da48-4af3-8850-da945149059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3652b79b-3003-4ee1-803b-b83f365bb997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor(15.),\n",
       " tensor([3., 5., 7.]),\n",
       " tensor([ 3., 12.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).view((2,3))\n",
    "A, A.sum(), A.sum(axis=0), A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "069851ec-372f-4659-99ba-c7e3d6c2f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]),\n",
       " tensor([[0.0000, 0.3333, 0.6667],\n",
       "         [0.2500, 0.3333, 0.4167]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape, A/sum_A # Using A/sum_A we were able to make the sum of all elements = 1 by using keepdims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ab15-e156-42e0-baa4-4b8e200199d8",
   "metadata": {},
   "source": [
    "### Dot Product and Cross Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70f91444-5e9a-495a-919d-ed58fe245d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6ff587e-7ee0-4584-939e-d1348c226691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.,  3.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b891539-b59e-42b4-a971-989a5b2ec03e",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430842f-cc1c-47c6-a0de-80da8552db67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
