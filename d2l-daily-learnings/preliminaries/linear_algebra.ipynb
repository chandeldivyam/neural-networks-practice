{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8b20e2-3d58-4b74-863b-70d1305ff7f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear Algebra\n",
    "\n",
    "We understand the importance of vectors, matrics and tensors and how to perform algebraic operations on them. Also, some relevant mathematical operations that will come handy during model preparation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5381a789-dff2-40a8-96d2-7274fb8174f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(3.)\n",
      "tensor([0, 1, 2]) torch.Size([3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x, y = torch.tensor(2), torch.tensor(3.)\n",
    "print(x, y) # Scalar\n",
    "\n",
    "x = torch.arange(3)\n",
    "print(x, x.shape) # Vectors\n",
    "\n",
    "X = torch.arange(6).view((2,3))\n",
    "print(X) # Matrics\n",
    "\n",
    "X_transpose = X.T\n",
    "print(X_transpose) # X_transpose_ij = X_ji\n",
    "\n",
    "X = torch.arange(24).view((2,3,4))\n",
    "print(X) # Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08c32-8205-49ad-a375-7926a4fc41c6",
   "metadata": {},
   "source": [
    "### Reduction and Non-Reduction of Tensor\n",
    "\n",
    "We can reduce the tensor along any axis with `sum()` function. Also, we can sum only along a particular axis and not reduce it completely.\n",
    "\n",
    "The `keepdims` parameter keeps the dimensions along differents axes, this helps when we want to normallize along some direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0e31c9-da48-4af3-8850-da945149059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3652b79b-3003-4ee1-803b-b83f365bb997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor(15.),\n",
       " tensor([3., 5., 7.]),\n",
       " tensor([ 3., 12.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).view((2,3))\n",
    "A, A.sum(), A.sum(axis=0), A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069851ec-372f-4659-99ba-c7e3d6c2f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]),\n",
       " tensor([[0.0000, 0.3333, 0.6667],\n",
       "         [0.2500, 0.3333, 0.4167]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape, A/sum_A # Using A/sum_A we were able to make the sum of all elements = 1 by using keepdims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ab15-e156-42e0-baa4-4b8e200199d8",
   "metadata": {},
   "source": [
    "### Dot Product and Cross Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f91444-5e9a-495a-919d-ed58fe245d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ff587e-7ee0-4584-939e-d1348c226691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.,  3.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b891539-b59e-42b4-a971-989a5b2ec03e",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0dece0-4db6-46b2-b492-dca8d73f3200",
   "metadata": {},
   "source": [
    "1. Prove that the transpose of the transpose of a matrix is the matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a734ca-ca73-4f49-9c42-6fb8ddc46aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((3,3))\n",
    "A_T = A.T\n",
    "A_T_T = A_T.T\n",
    "\n",
    "A == A_T_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caa964-cf62-4e31-ac89-390c7e6a4552",
   "metadata": {},
   "source": [
    "2. Given two matrices A and B, show that sum and transposition commute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a836011e-b6db-4fdf-b741-ef45001c8f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8139, -0.6017, -1.0057],\n",
       "         [ 1.2051,  1.2881,  1.1881],\n",
       "         [ 0.1341, -0.5966,  1.8152]]),\n",
       " tensor([[ 0.5449,  0.4095,  1.0028],\n",
       "         [-0.0169,  0.8721,  0.5611],\n",
       "         [ 1.5233, -0.6360, -1.9314]]),\n",
       " tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((3,3))\n",
    "B = torch.randn((3,3))\n",
    "\n",
    "A, B, (A.T + B.T) == (A + B).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3430456-1028-4646-b876-f4909c05dcd2",
   "metadata": {},
   "source": [
    "4. We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)? Write your answer without implementing any code, then check your answer using code.\n",
    "Ans: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a8af6a-be63-4ac0-afd8-0c177166eb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn((2,3,4))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08940575-dbd4-4590-82de-d68ff9305425",
   "metadata": {},
   "source": [
    "6. Run A / A.sum(axis=1) and see what happens. Can you analyze the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecebc55a-4f75-4380-b3ea-0eab4af03ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([ 3, 12, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0833, 0.0952],\n",
       "        [1.0000, 0.3333, 0.2381],\n",
       "        [2.0000, 0.5833, 0.3810]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(9).view((3,3))\n",
    "print(A)             # 3x3\n",
    "print(A.sum(axis=1)) # 3x1\n",
    "# A.sum will get broadcasted for division\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6bbb0-e524-4ac2-8532-36241d1768b3",
   "metadata": {},
   "source": [
    "8. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c08572-ea5a-4fc3-87c5-2d0fadec77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[12, 14, 16, 18],\n",
      "        [20, 22, 24, 26],\n",
      "        [28, 30, 32, 34]]) torch.Size([3, 4])\n",
      "tensor([[12, 15, 18, 21],\n",
      "        [48, 51, 54, 57]]) torch.Size([2, 4])\n",
      "tensor([[ 6, 22, 38],\n",
      "        [54, 70, 86]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(24).reshape((2,3,4))\n",
    "print(A)\n",
    "print(A.sum(axis=0), A.sum(axis=0).shape)\n",
    "print(A.sum(axis=1), A.sum(axis=1).shape)\n",
    "\n",
    "print(A.sum(axis=2), A.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884d5b5-f5ba-4fa7-8485-834c61ea28ca",
   "metadata": {},
   "source": [
    "Consider three large matrices, say A∈R2^10×2^16 , B∈B2^16×2^5 and C∈C2^5×2^14 , initialized with Gaussian random variables. You want to compute the product ABC:\n",
    "\n",
    "Ans: Response by BARD<br>\n",
    "Yes, there can be differences in memory footprint and speed depending on the order of matrix multiplications, even though the final result is the same.\n",
    "\n",
    "Here's a breakdown of the factors to consider:\n",
    "\n",
    "1. Memory Footprint:\n",
    "\n",
    "(AB)C:\n",
    "The intermediate product AB has dimensions 2^10 x 2^5, requiring about 2^15 elements to store.\n",
    "A(BC):\n",
    "The intermediate product BC has dimensions 2^16 x 2^14, requiring about 2^30 elements to store.\n",
    "Clearly, (AB)C consumes less memory during computation as it involves a smaller intermediate product.\n",
    "2. Speed:\n",
    "\n",
    "Number of floating-point operations (FLOPs): Both computations require the same total number of FLOPs (roughly 2^49).\n",
    "Cache utilization: The order of multiplications can affect how efficiently data is loaded and used from cache memory.\n",
    "(AB)C: It might be more cache-friendly, as the smaller intermediate product AB can fit better in cache during subsequent multiplication with C.\n",
    "3. Hardware and Algorithm Optimizations:\n",
    "\n",
    "Matrix libraries: Optimized libraries like BLAS often have specific algorithms for different matrix dimensions and orders, which can impact speed.\n",
    "Hardware parallelism: Some hardware architectures might be better suited for certain matrix multiplication orders.\n",
    "In conclusion:\n",
    "\n",
    "(AB)C is generally preferred for memory efficiency due to the smaller intermediate product.\n",
    "A(BC) might be faster in some cases due to better cache utilization or hardware-specific optimizations, but it's more memory-intensive.\n",
    "The optimal choice depends on the specific hardware, libraries, and matrix dimensions involved.\n",
    "It's always recommended to benchmark both options to determine the most efficient approach for a given scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e186ebb-8154-4e77-a410-d2c8e7174702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
