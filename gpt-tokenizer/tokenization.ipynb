{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80dd982-40f7-4938-8eaf-4e39cc406b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d0021f9-582f-4244-9c64-f915c255f584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean!)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"안녕하세요 👋 (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edce83a5-7aeb-47aa-92fc-6c4c1f6cdc71",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"안녕하세요 👋 (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55a2d81c-c03c-4a8a-87ea-5bfb059c40ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae65bd0-fafc-4e13-9302-aeb23b4ee1d6",
   "metadata": {},
   "source": [
    "## Byte Pairing Algorithm\n",
    "- We try to find repeating tokens and replace them with a new vocabulary word. This method helps reduce the amount of tokens which we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cafd5d8e-5b35-424c-9abf-e30a18a1159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616, 533)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to get a text and encode them utf-8 and get bytes data\n",
    "# convert them into a list of integers\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(tokens)\n",
    "len(tokens), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08e98de0-cc08-416d-8ae4-142d2e27ccbc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_frequent_byte_pair(token_list):\n",
    "    # Iterating over to get pair wisefrequency\n",
    "    assert isinstance(token_list, list), \"Please pass a list element\"\n",
    "    if not (len(token_list) > 1):\n",
    "        raise Exception(\"Please pass a list with elements more than 1\")\n",
    "        \n",
    "    token_pairs = zip(token_list, token_list[1:])\n",
    "    pair_counts = {}\n",
    "\n",
    "    for pair in token_pairs:\n",
    "        pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "stats = most_frequent_byte_pair(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a54cab0e-95a6-4587-b0f7-6d75e34ce962",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pair = sorted(((v,k) for k,v in stats.items()), reverse=True)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a369af23-23ea-4431-8ce8-b1c37717fda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c06d77-51f8-4833-b941-3efc5a9f424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i=0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids)-1 and ids[i] == pair[0] and  ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92136fa3-77c3-4a1d-b332-2afa34213af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We take longer text for better visual representation and understanding\n",
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7c78517-0f18-44a9-aa5f-c90fc82ac409",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into new token 256\n",
      "merging (105, 110) into new token 257\n",
      "merging (115, 32) into new token 258\n",
      "merging (116, 104) into new token 259\n",
      "merging (101, 114) into new token 260\n",
      "merging (99, 111) into new token 261\n",
      "merging (116, 32) into new token 262\n",
      "merging (226, 128) into new token 263\n",
      "merging (44, 32) into new token 264\n",
      "merging (97, 110) into new token 265\n",
      "merging (111, 114) into new token 266\n",
      "merging (100, 32) into new token 267\n",
      "merging (97, 114) into new token 268\n",
      "merging (101, 110) into new token 269\n",
      "merging (257, 103) into new token 270\n",
      "merging (261, 100) into new token 271\n",
      "merging (121, 32) into new token 272\n",
      "merging (46, 32) into new token 273\n",
      "merging (97, 108) into new token 274\n",
      "merging (259, 256) into new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "\n",
    "for i in range(num_merges):\n",
    "    stats = most_frequent_byte_pair(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce2fb708-2c81-4f58-8280-7f523f22514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 24597\n",
      "ids length: 19438\n",
      "compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531bc9a-8dfc-4847-9ff6-8ae92159da0d",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "- Given a token sequence, we go through the tokenizer and generate human text (pyton string object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2a081c7-062d-4a15-9fbb-14a932d4bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given list of integers, we will return a python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06b5b427-a3c1-406e-a33b-c6b2035dc548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a810878-e1d4-4677-b573-39c145df4c3a",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf201faa-0ea1-49bb-9617-83f9f81fcffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = most_frequent_byte_pair(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing to merge\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0565e84a-1c50-4f02-b12a-7ef923935652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = \"hello_world\"\n",
    "text_test == decode(encode(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e73d9-a603-43ff-bca0-398d68846597",
   "metadata": {},
   "source": [
    "## Forcing splits between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba7e3c9c-0f5a-4b2e-9094-96b63bb6cd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2040.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /root/projects/neural-networks-practice/env/lib/python3.10/site-packages (2024.5.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce2b84b1-55e7-43c9-a791-16f96b600e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ffe96-9860-45d7-bc9f-bf6a587bdc8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MinBPE Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ec95bb03-8818-42a5-9d48-114b0c174430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88691.66s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-21 20:43:22--  https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘taylorswift.txt’\n",
      "\n",
      "taylorswift.txt         [ <=>                ] 974.31K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-06-21 20:43:23 (10.4 MB/s) - ‘taylorswift.txt’ saved [997698]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814ad65-e807-46af-a295-ead0d5303f2c",
   "metadata": {},
   "source": [
    "## Step 1 - BasicTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a94bff88-8c6d-4e1b-942e-fc6e3de1d7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 256/244: (61, 34) -> 0 (b'=\"') had 13145 occurrences\n",
      "merge 257/244: (105, 110) -> 1 (b'in') had 12886 occurrences\n",
      "merge 258/244: (105, 118) -> 2 (b'iv') had 11933 occurrences\n",
      "merge 259/244: (101, 114) -> 3 (b'er') had 9728 occurrences\n",
      "merge 260/244: (34, 32) -> 4 (b'\" ') had 9689 occurrences\n",
      "merge 261/244: (101, 45) -> 5 (b'e-') had 9652 occurrences\n",
      "merge 262/244: (100, 258) -> 6 (b'div') had 9334 occurrences\n",
      "merge 263/244: (111, 110) -> 7 (b'on') had 9246 occurrences\n",
      "merge 264/244: (114, 101) -> 8 (b're') had 8773 occurrences\n",
      "merge 265/244: (97, 116) -> 9 (b'at') had 7570 occurrences\n",
      "merge 266/244: (101, 32) -> 10 (b'e ') had 6985 occurrences\n",
      "merge 267/244: (115, 116) -> 11 (b'st') had 6912 occurrences\n",
      "merge 268/244: (99, 116) -> 12 (b'ct') had 6820 occurrences\n",
      "merge 269/244: (62, 60) -> 13 (b'><') had 6719 occurrences\n",
      "merge 270/244: (99, 111) -> 14 (b'co') had 6376 occurrences\n",
      "merge 271/244: (44, 32) -> 15 (b', ') had 5978 occurrences\n",
      "merge 272/244: (116, 101) -> 16 (b'te') had 5822 occurrences\n",
      "merge 273/244: (32, 32) -> 17 (b'  ') had 5798 occurrences\n",
      "merge 274/244: (108, 257) -> 18 (b'lin') had 5720 occurrences\n",
      "merge 275/244: (111, 114) -> 19 (b'or') had 5640 occurrences\n",
      "merge 276/244: (97, 268) -> 20 (b'act') had 5532 occurrences\n",
      "merge 277/244: (100, 32) -> 21 (b'd ') had 5459 occurrences\n",
      "merge 278/244: (108, 101) -> 22 (b'le') had 5428 occurrences\n",
      "merge 279/244: (46, 32) -> 23 (b'. ') had 5127 occurrences\n",
      "merge 280/244: (262, 269) -> 24 (b'div><') had 5034 occurrences\n",
      "merge 281/244: (264, 276) -> 25 (b'react') had 5002 occurrences\n",
      "merge 282/244: (281, 45) -> 26 (b'react-') had 4972 occurrences\n",
      "merge 283/244: (97, 115) -> 27 (b'as') had 4921 occurrences\n",
      "merge 284/244: (50, 48) -> 28 (b'20') had 4910 occurrences\n",
      "merge 285/244: (98, 259) -> 29 (b'ber') had 4559 occurrences\n",
      "merge 286/244: (97, 114) -> 30 (b'ar') had 4369 occurrences\n",
      "merge 287/244: (97, 45) -> 31 (b'a-') had 4263 occurrences\n",
      "merge 288/244: (274, 261) -> 32 (b'line-') had 4131 occurrences\n",
      "merge 289/244: (116, 104) -> 33 (b'th') had 4122 occurrences\n",
      "merge 290/244: (109, 285) -> 34 (b'mber') had 4117 occurrences\n",
      "merge 291/244: (100, 265) -> 35 (b'dat') had 4073 occurrences\n",
      "merge 292/244: (105, 103) -> 36 (b'ig') had 4060 occurrences\n",
      "merge 293/244: (47, 280) -> 37 (b'/div><') had 4043 occurrences\n",
      "merge 294/244: (270, 100) -> 38 (b'cod') had 4021 occurrences\n",
      "merge 295/244: (291, 287) -> 39 (b'data-') had 4004 occurrences\n",
      "merge 296/244: (294, 261) -> 40 (b'code-') had 3996 occurrences\n",
      "merge 297/244: (283, 115) -> 41 (b'ass') had 3823 occurrences\n",
      "merge 298/244: (99, 108) -> 42 (b'cl') had 3805 occurrences\n",
      "merge 299/244: (97, 110) -> 43 (b'an') had 3787 occurrences\n",
      "merge 300/244: (32, 49) -> 44 (b' 1') had 3718 occurrences\n",
      "merge 301/244: (104, 116) -> 45 (b'ht') had 3638 occurrences\n",
      "merge 302/244: (298, 297) -> 46 (b'class') had 3566 occurrences\n",
      "merge 303/244: (302, 256) -> 47 (b'class=\"') had 3555 occurrences\n",
      "merge 304/244: (115, 32) -> 48 (b's ') had 3525 occurrences\n",
      "merge 305/244: (271, 284) -> 49 (b', 20') had 3410 occurrences\n",
      "merge 306/244: (101, 277) -> 50 (b'ed ') had 3299 occurrences\n",
      "merge 307/244: (110, 116) -> 51 (b'nt') had 3257 occurrences\n",
      "merge 308/244: (110, 117) -> 52 (b'nu') had 3246 occurrences\n",
      "merge 309/244: (105, 100) -> 53 (b'id') had 3200 occurrences\n",
      "merge 310/244: (262, 32) -> 54 (b'div ') had 3160 occurrences\n",
      "merge 311/244: (111, 116) -> 55 (b'ot') had 3148 occurrences\n",
      "merge 312/244: (121, 278) -> 56 (b'yle') had 3120 occurrences\n",
      "merge 313/244: (32, 48) -> 57 (b' 0') had 3120 occurrences\n",
      "merge 314/244: (97, 108) -> 58 (b'al') had 3118 occurrences\n",
      "merge 315/244: (267, 312) -> 59 (b'style') had 3061 occurrences\n",
      "merge 316/244: (308, 290) -> 60 (b'number') had 3037 occurrences\n",
      "merge 317/244: (315, 256) -> 61 (b'style=\"') had 3002 occurrences\n",
      "merge 318/244: (260, 317) -> 62 (b'\" style=\"') had 2993 occurrences\n",
      "merge 319/244: (282, 296) -> 63 (b'react-code-') had 2978 occurrences\n",
      "merge 320/244: (288, 316) -> 64 (b'line-number') had 2965 occurrences\n",
      "merge 321/244: (116, 32) -> 65 (b't ') had 2918 occurrences\n",
      "merge 322/244: (105, 116) -> 66 (b'it') had 2771 occurrences\n",
      "merge 323/244: (113, 117) -> 67 (b'qu') had 2666 occurrences\n",
      "merge 324/244: (105, 263) -> 68 (b'ion') had 2664 occurrences\n",
      "merge 325/244: (105, 108) -> 69 (b'il') had 2554 occurrences\n",
      "merge 326/244: (311, 59) -> 70 (b'ot;') had 2519 occurrences\n",
      "merge 327/244: (38, 323) -> 71 (b'&qu') had 2515 occurrences\n",
      "merge 328/244: (327, 326) -> 72 (b'&quot;') had 2515 occurrences\n",
      "merge 329/244: (34, 62) -> 73 (b'\">') had 2464 occurrences\n",
      "merge 330/244: (292, 301) -> 74 (b'ight') had 2462 occurrences\n",
      "merge 331/244: (257, 103) -> 75 (b'ing') had 2459 occurrences\n",
      "merge 332/244: (260, 295) -> 76 (b'\" data-') had 2423 occurrences\n",
      "merge 333/244: (108, 275) -> 77 (b'lor') had 2411 occurrences\n",
      "merge 334/244: (273, 273) -> 78 (b'    ') had 2368 occurrences\n",
      "merge 335/244: (32, 83) -> 79 (b' S') had 2358 occurrences\n",
      "merge 336/244: (289, 266) -> 80 (b'the ') had 2288 occurrences\n",
      "merge 337/244: (260, 303) -> 81 (b'\" class=\"') had 2281 occurrences\n",
      "merge 338/244: (116, 114) -> 82 (b'tr') had 2277 occurrences\n",
      "merge 339/244: (99, 101) -> 83 (b'ce') had 2268 occurrences\n",
      "merge 340/244: (117, 116) -> 84 (b'ut') had 2257 occurrences\n",
      "merge 341/244: (272, 120) -> 85 (b'tex') had 2235 occurrences\n",
      "merge 342/244: (97, 121) -> 86 (b'ay') had 2207 occurrences\n",
      "merge 343/244: (330, 58) -> 87 (b'ight:') had 2186 occurrences\n",
      "merge 344/244: (309, 256) -> 88 (b'id=\"') had 2169 occurrences\n",
      "merge 345/244: (105, 101) -> 89 (b'ie') had 2104 occurrences\n",
      "merge 346/244: (119, 105) -> 90 (b'wi') had 2093 occurrences\n",
      "merge 347/244: (46, 55) -> 91 (b'.7') had 2028 occurrences\n",
      "merge 348/244: (293, 293) -> 92 (b'/div></div><') had 2026 occurrences\n",
      "merge 349/244: (99, 263) -> 93 (b'con') had 2010 occurrences\n",
      "merge 350/244: (114, 111) -> 94 (b'ro') had 1993 occurrences\n",
      "merge 351/244: (293, 310) -> 95 (b'/div><div ') had 1992 occurrences\n",
      "merge 352/244: (97, 100) -> 96 (b'ad') had 1979 occurrences\n",
      "merge 353/244: (318, 112) -> 97 (b'\" style=\"p') had 1978 occurrences\n",
      "merge 354/244: (320, 256) -> 98 (b'line-number=\"') had 1976 occurrences\n",
      "merge 355/244: (337, 282) -> 99 (b'\" class=\"react-') had 1976 occurrences\n",
      "merge 356/244: (319, 341) -> 100 (b'react-code-tex') had 1976 occurrences\n",
      "merge 357/244: (121, 32) -> 101 (b'y ') had 1874 occurrences\n",
      "merge 358/244: (32, 336) -> 102 (b' the ') had 1836 occurrences\n",
      "merge 359/244: (99, 104) -> 103 (b'ch') had 1828 occurrences\n",
      "merge 360/244: (346, 102) -> 104 (b'wif') had 1771 occurrences\n",
      "merge 361/244: (82, 101) -> 105 (b'Re') had 1740 occurrences\n",
      "merge 362/244: (92, 34) -> 106 (b'\\\\\"') had 1677 occurrences\n",
      "merge 363/244: (49, 54) -> 107 (b'16') had 1669 occurrences\n",
      "merge 364/244: (46, 50) -> 108 (b'.2') had 1660 occurrences\n",
      "merge 365/244: (104, 101) -> 109 (b'he') had 1540 occurrences\n",
      "merge 366/244: (112, 120) -> 110 (b'px') had 1537 occurrences\n",
      "merge 367/244: (105, 99) -> 111 (b'ic') had 1481 occurrences\n",
      "merge 368/244: (108, 108) -> 112 (b'll') had 1420 occurrences\n",
      "merge 369/244: (335, 360) -> 113 (b' Swif') had 1417 occurrences\n",
      "merge 370/244: (314, 32) -> 114 (b'al ') had 1413 occurrences\n",
      "merge 371/244: (34, 269) -> 115 (b'\"><') had 1406 occurrences\n",
      "merge 372/244: (305, 49) -> 116 (b', 201') had 1390 occurrences\n",
      "merge 373/244: (347, 53) -> 117 (b'.75') had 1364 occurrences\n",
      "merge 374/244: (272, 307) -> 118 (b'tent') had 1353 occurrences\n",
      "merge 375/244: (305, 50) -> 119 (b', 202') had 1346 occurrences\n",
      "merge 376/244: (263, 32) -> 120 (b'on ') had 1332 occurrences\n",
      "merge 377/244: (313, 313) -> 121 (b' 0 0') had 1332 occurrences\n",
      "merge 378/244: (342, 333) -> 122 (b'aylor') had 1329 occurrences\n",
      "merge 379/244: (111, 115) -> 123 (b'os') had 1308 occurrences\n",
      "merge 380/244: (118, 306) -> 124 (b'ved ') had 1301 occurrences\n",
      "merge 381/244: (84, 378) -> 125 (b'Taylor') had 1294 occurrences\n",
      "merge 382/244: (279, 361) -> 126 (b'. Re') had 1294 occurrences\n",
      "merge 383/244: (109, 257) -> 127 (b'min') had 1292 occurrences\n",
      "merge 384/244: (345, 380) -> 128 (b'ieved ') had 1282 occurrences\n",
      "merge 385/244: (382, 338) -> 129 (b'. Retr') had 1278 occurrences\n",
      "merge 386/244: (385, 384) -> 130 (b'. Retrieved ') had 1278 occurrences\n",
      "merge 387/244: (105, 115) -> 131 (b'is') had 1275 occurrences\n",
      "merge 388/244: (349, 374) -> 132 (b'content') had 1266 occurrences\n",
      "merge 389/244: (101, 110) -> 133 (b'en') had 1257 occurrences\n",
      "merge 390/244: (108, 265) -> 134 (b'lat') had 1253 occurrences\n",
      "merge 391/244: (259, 32) -> 135 (b'er ') had 1252 occurrences\n",
      "merge 392/244: (100, 331) -> 136 (b'ding') had 1247 occurrences\n",
      "merge 393/244: (258, 101) -> 137 (b'ive') had 1240 occurrences\n",
      "merge 394/244: (324, 58) -> 138 (b'ion:') had 1225 occurrences\n",
      "merge 395/244: (44, 91) -> 139 (b',[') had 1223 occurrences\n",
      "merge 396/244: (381, 369) -> 140 (b'Taylor Swif') had 1196 occurrences\n",
      "merge 397/244: (108, 45) -> 141 (b'l-') had 1185 occurrences\n",
      "merge 398/244: (364, 53) -> 142 (b'.25') had 1182 occurrences\n",
      "merge 399/244: (32, 50) -> 143 (b' 2') had 1159 occurrences\n",
      "merge 400/244: (379, 322) -> 144 (b'osit') had 1141 occurrences\n",
      "merge 401/244: (45, 114) -> 145 (b'-r') had 1124 occurrences\n",
      "merge 402/244: (111, 117) -> 146 (b'ou') had 1123 occurrences\n",
      "merge 403/244: (102, 325) -> 147 (b'fil') had 1116 occurrences\n",
      "merge 404/244: (352, 392) -> 148 (b'adding') had 1113 occurrences\n",
      "merge 405/244: (111, 32) -> 149 (b'o ') had 1109 occurrences\n",
      "merge 406/244: (97, 340) -> 150 (b'aut') had 1100 occurrences\n",
      "merge 407/244: (383, 45) -> 151 (b'min-') had 1100 occurrences\n",
      "merge 408/244: (46, 53) -> 152 (b'.5') had 1088 occurrences\n",
      "merge 409/244: (272, 267) -> 153 (b'test') had 1085 occurrences\n",
      "merge 410/244: (406, 111) -> 154 (b'auto') had 1084 occurrences\n",
      "merge 411/244: (365, 343) -> 155 (b'height:') had 1073 occurrences\n",
      "merge 412/244: (401, 343) -> 156 (b'-right:') had 1071 occurrences\n",
      "merge 413/244: (350, 109) -> 157 (b'rom') had 1068 occurrences\n",
      "merge 414/244: (34, 44) -> 158 (b'\",') had 1065 occurrences\n",
      "merge 415/244: (264, 390) -> 159 (b'relat') had 1062 occurrences\n",
      "merge 416/244: (115, 99) -> 160 (b'sc') had 1050 occurrences\n",
      "merge 417/244: (41, 279) -> 161 (b'). ') had 1048 occurrences\n",
      "merge 418/244: (274, 266) -> 162 (b'line ') had 1045 occurrences\n",
      "merge 419/244: (409, 344) -> 163 (b'testid=\"') had 1030 occurrences\n",
      "merge 420/244: (332, 419) -> 164 (b'\" data-testid=\"') had 1025 occurrences\n",
      "merge 421/244: (299, 277) -> 165 (b'and ') had 1019 occurrences\n",
      "merge 422/244: (400, 394) -> 166 (b'osition:') had 1012 occurrences\n",
      "merge 423/244: (403, 261) -> 167 (b'file-') had 1011 occurrences\n",
      "merge 424/244: (102, 413) -> 168 (b'from') had 1009 occurrences\n",
      "merge 425/244: (292, 257) -> 169 (b'igin') had 1008 occurrences\n",
      "merge 426/244: (65, 114) -> 170 (b'Ar') had 1007 occurrences\n",
      "merge 427/244: (351, 303) -> 171 (b'/div><div class=\"') had 1002 occurrences\n",
      "merge 428/244: (415, 393) -> 172 (b'relative') had 1001 occurrences\n",
      "merge 429/244: (310, 344) -> 173 (b'div id=\"') had 1001 occurrences\n",
      "merge 430/244: (363, 366) -> 174 (b'16px') had 999 occurrences\n",
      "merge 431/244: (422, 428) -> 175 (b'osition:relative') had 999 occurrences\n",
      "merge 432/244: (404, 412) -> 176 (b'adding-right:') had 998 occurrences\n",
      "merge 433/244: (301, 109) -> 177 (b'htm') had 995 occurrences\n",
      "merge 434/244: (76, 67) -> 178 (b'LC') had 993 occurrences\n",
      "merge 435/244: (407, 411) -> 179 (b'min-height:') had 993 occurrences\n",
      "merge 436/244: (339, 368) -> 180 (b'cell') had 993 occurrences\n",
      "merge 437/244: (60, 348) -> 181 (b'</div></div><') had 992 occurrences\n",
      "merge 438/244: (275, 425) -> 182 (b'origin') had 989 occurrences\n",
      "merge 439/244: (433, 397) -> 183 (b'html-') had 989 occurrences\n",
      "merge 440/244: (432, 430) -> 184 (b'adding-right:16px') had 989 occurrences\n",
      "merge 441/244: (437, 427) -> 185 (b'</div></div></div><div class=\"') had 989 occurrences\n",
      "merge 442/244: (388, 115) -> 186 (b'contents') had 989 occurrences\n",
      "merge 443/244: (410, 371) -> 187 (b'auto\"><') had 989 occurrences\n",
      "merge 444/244: (295, 354) -> 188 (b'data-line-number=\"') had 988 occurrences\n",
      "merge 445/244: (355, 320) -> 189 (b'\" class=\"react-line-number') had 988 occurrences\n",
      "merge 446/244: (445, 32) -> 190 (b'\" class=\"react-line-number ') had 988 occurrences\n",
      "merge 447/244: (446, 356) -> 191 (b'\" class=\"react-line-number react-code-tex') had 988 occurrences\n",
      "merge 448/244: (447, 116) -> 192 (b'\" class=\"react-line-number react-code-text') had 988 occurrences\n",
      "merge 449/244: (448, 353) -> 193 (b'\" class=\"react-line-number react-code-text\" style=\"p') had 988 occurrences\n",
      "merge 450/244: (449, 440) -> 194 (b'\" class=\"react-line-number react-code-text\" style=\"padding-right:16px') had 988 occurrences\n",
      "merge 451/244: (450, 329) -> 195 (b'\" class=\"react-line-number react-code-text\" style=\"padding-right:16px\">') had 988 occurrences\n",
      "merge 452/244: (356, 321) -> 196 (b'react-code-text ') had 988 occurrences\n",
      "merge 453/244: (452, 319) -> 197 (b'react-code-text react-code-') had 988 occurrences\n",
      "merge 454/244: (453, 288) -> 198 (b'react-code-text react-code-line-') had 988 occurrences\n",
      "merge 455/244: (454, 442) -> 199 (b'react-code-text react-code-line-contents') had 988 occurrences\n",
      "merge 456/244: (455, 318) -> 200 (b'react-code-text react-code-line-contents\" style=\"') had 988 occurrences\n",
      "merge 457/244: (456, 435) -> 201 (b'react-code-text react-code-line-contents\" style=\"min-height:') had 988 occurrences\n",
      "merge 458/244: (457, 443) -> 202 (b'react-code-text react-code-line-contents\" style=\"min-height:auto\"><') had 988 occurrences\n",
      "merge 459/244: (458, 280) -> 203 (b'react-code-text react-code-line-contents\" style=\"min-height:auto\"><div><') had 988 occurrences\n",
      "merge 460/244: (459, 429) -> 204 (b'react-code-text react-code-line-contents\" style=\"min-height:auto\"><div><div id=\"') had 988 occurrences\n",
      "merge 461/244: (460, 434) -> 205 (b'react-code-text react-code-line-contents\" style=\"min-height:auto\"><div><div id=\"LC') had 988 occurrences\n",
      "merge 462/244: (355, 423) -> 206 (b'\" class=\"react-file-') had 988 occurrences\n",
      "merge 463/244: (462, 418) -> 207 (b'\" class=\"react-file-line ') had 988 occurrences\n",
      "merge 464/244: (463, 439) -> 208 (b'\" class=\"react-file-line html-') had 988 occurrences\n",
      "merge 465/244: (464, 262) -> 209 (b'\" class=\"react-file-line html-div') had 988 occurrences\n",
      "merge 466/244: (465, 420) -> 210 (b'\" class=\"react-file-line html-div\" data-testid=\"') had 988 occurrences\n",
      "merge 467/244: (466, 296) -> 211 (b'\" class=\"react-file-line html-div\" data-testid=\"code-') had 988 occurrences\n",
      "merge 468/244: (467, 436) -> 212 (b'\" class=\"react-file-line html-div\" data-testid=\"code-cell') had 988 occurrences\n",
      "merge 469/244: (468, 332) -> 213 (b'\" class=\"react-file-line html-div\" data-testid=\"code-cell\" data-') had 988 occurrences\n",
      "merge 470/244: (469, 354) -> 214 (b'\" class=\"react-file-line html-div\" data-testid=\"code-cell\" data-line-number=\"') had 988 occurrences\n",
      "merge 471/244: (353, 431) -> 215 (b'\" style=\"position:relative') had 988 occurrences\n",
      "merge 472/244: (471, 329) -> 216 (b'\" style=\"position:relative\">') had 988 occurrences\n",
      "merge 473/244: (93, 395) -> 217 (b'],[') had 987 occurrences\n",
      "merge 474/244: (60, 351) -> 218 (b'</div><div ') had 987 occurrences\n",
      "merge 475/244: (474, 444) -> 219 (b'</div><div data-line-number=\"') had 987 occurrences\n",
      "merge 476/244: (441, 461) -> 220 (b'</div></div></div><div class=\"react-code-text react-code-line-contents\" style=\"min-height:auto\"><div><div id=\"LC') had 987 occurrences\n",
      "merge 477/244: (473, 473) -> 221 (b'],[],[') had 986 occurrences\n",
      "merge 478/244: (97, 32) -> 222 (b'a ') had 981 occurrences\n",
      "merge 479/244: (111, 102) -> 223 (b'of') had 954 occurrences\n",
      "merge 480/244: (51, 57) -> 224 (b'39') had 947 occurrences\n",
      "merge 481/244: (263, 101) -> 225 (b'one') had 924 occurrences\n",
      "merge 482/244: (258, 306) -> 226 (b'ived ') had 910 occurrences\n",
      "merge 483/244: (45, 98) -> 227 (b'-b') had 901 occurrences\n",
      "merge 484/244: (100, 101) -> 228 (b'de') had 898 occurrences\n",
      "merge 485/244: (424, 358) -> 229 (b'from the ') had 894 occurrences\n",
      "merge 486/244: (438, 370) -> 230 (b'original ') had 892 occurrences\n",
      "merge 487/244: (115, 101) -> 231 (b'se') had 888 occurrences\n",
      "merge 488/244: (104, 266) -> 232 (b'he ') had 883 occurrences\n",
      "merge 489/244: (426, 359) -> 233 (b'Arch') had 880 occurrences\n",
      "merge 490/244: (489, 482) -> 234 (b'Archived ') had 880 occurrences\n",
      "merge 491/244: (485, 486) -> 235 (b'from the original ') had 880 occurrences\n",
      "merge 492/244: (490, 491) -> 236 (b'Archived from the original ') had 878 occurrences\n",
      "merge 493/244: (279, 492) -> 237 (b'. Archived from the original ') had 868 occurrences\n",
      "merge 494/244: (117, 115) -> 238 (b'us') had 865 occurrences\n",
      "merge 495/244: (109, 101) -> 239 (b'me') had 863 occurrences\n",
      "merge 496/244: (117, 108) -> 240 (b'ul') had 818 occurrences\n",
      "merge 497/244: (10, 334) -> 241 (b'\\n    ') had 816 occurrences\n",
      "merge 498/244: (493, 376) -> 242 (b'. Archived from the original on ') had 802 occurrences\n",
      "merge 499/244: (111, 118) -> 243 (b'ov') had 801 occurrences\n"
     ]
    }
   ],
   "source": [
    "class BasicTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer class like GPT4 but without regex and breaking the text    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.vocab = self._generate_vocab()\n",
    "\n",
    "    def encode(self, text):\n",
    "        assert isinstance(text, str) and len(text) >= 1, \"Please pass a valid text string\"\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) > 1:\n",
    "            stats = most_frequent_byte_pair(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def decode(self, ids):\n",
    "        encoded_bytes = \"\"\n",
    "        for idx in ids:\n",
    "            encoded_bytes += self.vocab.get(idx,b\"\").decode(\"utf-8\", errors=\"replace\")\n",
    "        return encoded_bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_vocab = vocab_size - 256\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        merges = {}\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        for idx in range(num_vocab):\n",
    "            stats = most_frequent_byte_pair(tokens)\n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            merges[idx+256] = max_pair\n",
    "            tokens = merge(tokens, max_pair, idx+256)\n",
    "            vocab[idx+256] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "            if verbose:\n",
    "                print(f\"merge {idx+256}/{num_vocab}: {max_pair} -> {idx} ({vocab[idx+256]}) had {stats[max_pair]} occurrences\")\n",
    "            \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "        return\n",
    "\n",
    "    def read_text_from_file(self, filename):\n",
    "        try:\n",
    "            with open(filename,  encoding='utf-8') as f:\n",
    "                self.training_text = f.read()\n",
    "            return {\"success\": True}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.training_text = None\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "    def _generate_vocab(self):\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        for (p1, p2), id_new in self.merges.items():\n",
    "            vocab[id_new] = vocab[p1] + vocab[p2]\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "basic_tokenizer.read_text_from_file(\"taylorswift.txt\")\n",
    "basic_tokenizer.train(basic_tokenizer.training_text, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a10aea2d-f521-40af-b8f4-90d6394bdbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = \"Hi, how are you?\"\n",
    "encoding = basic_tokenizer.encode(text_test)\n",
    "basic_tokenizer.decode(encoding) == text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c320c-6e36-4be8-b822-4ad64d796fa1",
   "metadata": {},
   "source": [
    "## Step 2 - RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d90d7bb-f3f4-408b-9d52-b098f3e4fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3fd826a7-5c07-4939-a122-ffef5b999a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer class like GPT4 with regex and breaking the text    \n",
    "    \"\"\"\n",
    "    def __init__(self, regex_pattern=None):\n",
    "        self.merges = {}\n",
    "        self.regex_pattern = GPT4_SPLIT_PATTERN if regex_pattern is None else regex_pattern\n",
    "\n",
    "    def encode(self, text):\n",
    "        assert isinstance(text, str) and len(text) >= 1, \"Please pass a valid text string\"\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) > 1:\n",
    "            stats = most_frequent_byte_pair(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        encoded_bytes = \"\"\n",
    "        for idx in ids:\n",
    "            encoded_bytes += self.vocab.get(idx,b\"\").decode(\"utf-8\", errors=\"replace\")\n",
    "        return encoded_bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        num_vocab = vocab_size - 256\n",
    "        merges = {}\n",
    "        text_chunks = re.findall(re.compile(self.regex_pattern), text)\n",
    "        text_ids = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
    "        for i in range(num_vocab):\n",
    "            stats = {}\n",
    "            for chunk_id in text_ids:\n",
    "                if len(chunk_id) < 2: continue\n",
    "                chunk_stats = most_frequent_byte_pair(chunk_id)\n",
    "                for k,v in chunk_stats.items():\n",
    "                    stats[k] = stats[k] + v if stats.get(k) else v\n",
    "\n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            text_ids = self._replace_pair(text_ids, list(max_pair), [i+256])\n",
    "            vocab[i+256] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "            merges[i+256] = max_pair\n",
    "            if verbose:\n",
    "                print(f\"merge {i+256}/{num_vocab}: {max_pair} -> {i} ({vocab[i+256]}) had {stats[max_pair]} occurrences\")\n",
    "\n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "        return\n",
    "\n",
    "    def read_text_from_file(self, filename):\n",
    "        try:\n",
    "            with open(filename,  encoding='utf-8') as f:\n",
    "                self.training_text = f.read()\n",
    "            return {\"success\": True}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.training_text = None\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "    def _replace_pair(self, tokens, pair, replacement):\n",
    "        result = []\n",
    "        pair_len = len(pair)\n",
    "        \n",
    "        for sublist in tokens:\n",
    "            i = 0\n",
    "            new_sublist = []\n",
    "            while i < len(sublist):\n",
    "                # Check if the current slice matches the pair\n",
    "                if sublist[i:i + pair_len] == pair:\n",
    "                    new_sublist.extend(replacement)\n",
    "                    i += pair_len\n",
    "                else:\n",
    "                    new_sublist.append(sublist[i])\n",
    "                    i += 1\n",
    "            result.append(new_sublist)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1e9680fd-e69c-4d04-9ac7-4e62a2b28d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 256/44: (61, 34) -> 0 (b'=\"') had 13145 occurrences\n",
      "merge 257/44: (105, 110) -> 1 (b'in') had 12886 occurrences\n",
      "merge 258/44: (105, 118) -> 2 (b'iv') had 11933 occurrences\n",
      "merge 259/44: (101, 114) -> 3 (b'er') had 9728 occurrences\n",
      "merge 260/44: (100, 258) -> 4 (b'div') had 9334 occurrences\n",
      "merge 261/44: (111, 110) -> 5 (b'on') had 9246 occurrences\n",
      "merge 262/44: (114, 101) -> 6 (b're') had 8798 occurrences\n",
      "merge 263/44: (97, 116) -> 7 (b'at') had 7570 occurrences\n",
      "merge 264/44: (115, 116) -> 8 (b'st') had 6912 occurrences\n",
      "merge 265/44: (108, 101) -> 9 (b'le') had 6907 occurrences\n",
      "merge 266/44: (99, 116) -> 10 (b'ct') had 6820 occurrences\n",
      "merge 267/44: (62, 60) -> 11 (b'><') had 6719 occurrences\n",
      "merge 268/44: (99, 111) -> 12 (b'co') had 6376 occurrences\n",
      "merge 269/44: (116, 101) -> 13 (b'te') had 6004 occurrences\n",
      "merge 270/44: (108, 257) -> 14 (b'lin') had 5720 occurrences\n",
      "merge 271/44: (100, 101) -> 15 (b'de') had 5678 occurrences\n",
      "merge 272/44: (111, 114) -> 16 (b'or') had 5635 occurrences\n",
      "merge 273/44: (97, 266) -> 17 (b'act') had 5532 occurrences\n",
      "merge 274/44: (270, 101) -> 18 (b'line') had 5313 occurrences\n",
      "merge 275/44: (262, 273) -> 19 (b'react') had 5002 occurrences\n",
      "merge 276/44: (45, 274) -> 20 (b'-line') had 4957 occurrences\n",
      "merge 277/44: (104, 101) -> 21 (b'he') had 4922 occurrences\n",
      "merge 278/44: (97, 115) -> 22 (b'as') had 4921 occurrences\n",
      "merge 279/44: (32, 32) -> 23 (b'  ') had 4856 occurrences\n",
      "merge 280/44: (98, 259) -> 24 (b'ber') had 4559 occurrences\n",
      "merge 281/44: (50, 48) -> 25 (b'20') had 4553 occurrences\n",
      "merge 282/44: (97, 114) -> 26 (b'ar') had 4368 occurrences\n",
      "merge 283/44: (32, 99) -> 27 (b' c') had 4283 occurrences\n",
      "merge 284/44: (32, 100) -> 28 (b' d') had 4193 occurrences\n",
      "merge 285/44: (109, 280) -> 29 (b'mber') had 4117 occurrences\n",
      "merge 286/44: (105, 103) -> 30 (b'ig') had 4060 occurrences\n",
      "merge 287/44: (263, 97) -> 31 (b'ata') had 4054 occurrences\n",
      "merge 288/44: (268, 271) -> 32 (b'code') had 4017 occurrences\n",
      "merge 289/44: (32, 116) -> 33 (b' t') had 3949 occurrences\n",
      "merge 290/44: (278, 115) -> 34 (b'ass') had 3823 occurrences\n",
      "merge 291/44: (97, 110) -> 35 (b'an') had 3787 occurrences\n",
      "merge 292/44: (104, 116) -> 36 (b'ht') had 3640 occurrences\n",
      "merge 293/44: (101, 100) -> 37 (b'ed') had 3638 occurrences\n",
      "merge 294/44: (283, 108) -> 38 (b' cl') had 3591 occurrences\n",
      "merge 295/44: (294, 290) -> 39 (b' class') had 3563 occurrences\n",
      "merge 296/44: (284, 287) -> 40 (b' data') had 3558 occurrences\n",
      "merge 297/44: (105, 116) -> 41 (b'it') had 3541 occurrences\n",
      "merge 298/44: (32, 83) -> 42 (b' S') had 3316 occurrences\n",
      "merge 299/44: (110, 116) -> 43 (b'nt') had 3277 occurrences\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.read_text_from_file(\"taylorswift.txt\")\n",
    "regex_tokenizer.train(basic_tokenizer.training_text, 300, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "482c0956-d521-4ea1-9426-e964b6d55080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = \"Hi, how are you?\"\n",
    "encoding = regex_tokenizer.encode(text_test)\n",
    "regex_tokenizer.decode(encoding) == text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f1b4e-2040-4b89-8bf9-6c874984ff7d",
   "metadata": {},
   "source": [
    "# Sentencepiece\n",
    "- Sentencepiece is used by both llama and mistral. This is good for both training and inference. They think differently about order.\n",
    "    - Ticktoken: Codepoints → string → encode to utf-8 → Bytes\n",
    "    - Sentencepiece: Works directly at the level of codepoints (BPE runs directly at the level of codepoints). It uses BPE on codepoints and then falls back to bytes in rare conditions\n",
    "- Andrej’s preference for normalization for raw text is that in DL we should try to not touch the data as much as possible. Like double spaces, or other things like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffa459-7685-426f-8f69-5dce975e16c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
